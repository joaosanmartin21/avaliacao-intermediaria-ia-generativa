# Frontend: deixe vazio para usar o proxy /api no Vite
VITE_ASSISTANT_API_URL=

# Ollama local (OpenAI-compatible endpoint)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_API_KEY=ollama

# Parametros do modelo
OLLAMA_TEMPERATURE=0.2
OLLAMA_TOP_P=0.9
OLLAMA_MAX_TOKENS=500
ASSISTANT_MAX_TOOL_ITERATIONS=4
